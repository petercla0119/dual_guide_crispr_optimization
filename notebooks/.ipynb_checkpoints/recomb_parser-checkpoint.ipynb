{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"dual_guide_parser_v10ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1WXHtAI4zVKt6PZPIYEvLqe4IpDI8Afb1\n",
    "\n",
    "# dual guide parser - December 2021\n",
    " - **Project:** iNDI.\n",
    " - **Author(s):** Dan Ramos, Lirong Peng, Faraz Faghri, Mike Nalls, and Nicholas Johnson\n",
    "\n",
    "---\n",
    "### Quick Description:\n",
    "- **Problem:** We need a method that preprocesses guides for experiments, something that parses fastqs to only include those guides included in R1 and R1 from the concensus guide list ... we also need to catalogue failed reads. We also need this to take MiSeq and other data.\n",
    "- **Solution:** The workflow below sums it up pretty well. Let's test out some code on small iNDI datasets provided by Dan R. We have added support from SeqIO\n",
    "\n",
    "### Workflow:\n",
    "0.   Set up notebook.\n",
    "1.   Import data, this includes concensus guides and R1 + R2 fastqs.\n",
    "2.   Filter out poor quality reads.\n",
    "3.   Detect whether any guide or read truncation is necessary.\n",
    "4.   Identify matching read groups across R1 and R2.\n",
    "5.   Reduce the datasets to the read groups that match in R1 and R2.\n",
    "6.   Split into 'hits.\\'  and 'recombinants.\\'.\n",
    "'hits.\\' denotes read group matches and the protospacers match.\n",
    "'recombinants.\\' denotes read group matches but one or more protospacers does not.\n",
    "7.   Export 'hits.\\' and 'recombinants.\\' per fastq.\n",
    "\n",
    "### Notes on data for testing:\n",
    "- **20200513_library_1_2_unbalanced_dJR051.csv** = All elements of the dual sgRNA library. Sequence from protospacer_A and protospacer_B columns must be present in the same row to be considered a match.\n",
    "- **UDP0011_S5_R1_001.fastq.gz** = Final 19 bases of each read should match the final 19 bases of \"protospacer_A\" sequence from \"20200513_library_1_2_unbalanced_dJR051.csv\".\n",
    "- **UDP0011_S5_R2_001.fastq.gz** = First 20 bases of each read should match reverse complement of \"protospacer_B\" sequence from \"20200513_library_1_2_unbalanced_dJR051.csv\".\n",
    "** The major change in V3 of this code is matching sequences for the guides using all UPPER CASE bases intead of being case-sensitive.\"\n",
    "** V8 automatically detects whether the first base of the reads and/or guides is G and acts accordingly. The result may be essentially running 19bp matches. Users may also set manually.\n",
    "** V9 adds the ability to match guide 2 to read 1 and the reverse complement of guide 1 to read 2 in addition to the standard workflow.\n",
    "** The above is also true for read 2.\n",
    "\n",
    "# 0.   Set up notebook.\n",
    "\"\"\"\n"
   ],
   "id": "9627dbc66c340f86"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# %% Set up notebook\n",
    "\n",
    "import os\n",
    "# from google.colab import drive\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "# import joblib\n",
    "import subprocess\n",
    "import argparse\n",
    "import gzip\n",
    "import textwrap\n",
    "import warnings\n",
    "\n",
    "#!pip install --upgrade tables\n",
    "#! pip install biopython\n",
    "\n",
    "# import tables\n",
    "# import requests\n",
    "# from Bio import SeqIO\n",
    "from Bio.Seq import reverse_complement\n",
    "from Bio.SeqIO.QualityIO import FastqGeneralIterator\n",
    "from itertools import islice\n",
    "# Comment out below after testing.\n",
    "\n",
    "#drive.mount('/content/drive/')\n",
    "#os.chdir(\"/content/drive/Shared drives/CARD_iNDI/scratch/dual_guide_parser\")\n",
    "# ! pwd\n",
    "\n",
    "# Set  options for testing.\n",
    "#\n",
    "guides_file = \"/Users/Claire/Downloads/git_clones/dual_guide_crispr_optimization/parser_files/20200513_library_1_2_unbalanced_dJR051.txt\"\n",
    "r1_file = \"/Users/Claire/Downloads/raw_sequencing/JH8105_1_S1_L001_R1_001.fastq.gz\"\n",
    "r2_file = \"/Users/Claire/Downloads/raw_sequencing/JH8105_1_S1_L001_R2_001.fastq.gz\"\n",
    "N_rows = 1500 # Speed up testing, this just reads the first 10K sequences.\n",
    "check_length = 500 # top and bottom of the array, how far to check for whether composed with G.\n",
    "guide_1_offset = -999 # -999 is the sentinel value\n",
    "guide_2_offset = -999 # -999 is the sentinel value\n",
    "read_1_offset = -999 # -999 is the sentinel value\n",
    "read_2_offset = -999 # -999 is the sentinel value\n",
    "purity = 0.95\n",
    "check_reverse = True\n",
    "\n",
    "# Set the options for production.\n",
    "\n",
    "# parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter, description=textwrap.dedent('''\\\n",
    "# \n",
    "# #Thanks for trying the dual_guide_parser from CARD + iNDI + DTi.\n",
    "# #To run this code, you will need to specify the guides_file, this is a file similar to the example 20200513_library_1_2_unbalanced_dJR051.csv.\n",
    "# #You will need to specify a pair of R1 and R2 files, such as UDP0007_S1_R1_001.fastq.gz and UDP0007_S1_R2_001.fastq.gz.\n",
    "# #You can also specify the number of read groups you are interested for testing the tool, this relates to the option n_groups.\n",
    "# #This will only read that many readgroups from the R1 and R2 files, allowing you to speed things up a bit.\n",
    "# #This code must be run from the working directory that contains the R1 and R2 files, but the guides_file can be anywhere, just\n",
    "# #specify a full path to the guides file like ~/Desktop/20200513_library_1_2_unbalanced_dJR051.csv.\n",
    "# #This code attempts to automatically equalize guides and reads. In other words,\n",
    "# #it will attempt to cut out letters that begin a guide as a rule, or a read. You may also specify these settings\n",
    "# #manually.\n",
    "# #This is best run on a large RAM / high CPU set up as the files are quite large.\n",
    "# #Finally, to run this code, you will need several packages, including biopython. To see the required packages listed, run with the -h option.\n",
    "# #\n",
    "# #'''))\n",
    "# parser.add_argument('--packages', help='Request for packages required to run, and how to install.', action='store_true')\n",
    "# parser.add_argument('--guides_file', type=str, default='missing', help='Mandatory input filepath. This is a file similar to the example 20200513_library_1_2_unbalanced_dJR051.csv. This can be a complete filepath')\n",
    "# parser.add_argument('--r1_file', type=str, default='missing', help='Mandatory input file name. An R1 file in your working directory.')\n",
    "# parser.add_argument('--r2_file', type=str, default='missing', help='Mandatory input file name. An R2 file in your working directory.')\n",
    "# parser.add_argument('--N_reads', type=int, default=0, help='Optional number of readgroups to test. An integer.')\n",
    "# parser.add_argument('--guide_1_offset', type=int, default = -999, help='# of characters to truncate from the left of the protospacer_A. Read truncation is not automatic, set accordingly. Default = 0.')\n",
    "# parser.add_argument('--read_1_offset', type=int, default = -999, help='# of characters to truncate from the left of read_1. Read truncation is not automatic, set accordingly. Default = 0.')\n",
    "# parser.add_argument('--guide_2_offset', type=int, default = -999, help='# of characters to truncate from the left of the protospacer_B. A Read truncation is not automatic, set accordingly. Default = 0.')\n",
    "# parser.add_argument('--read_2_offset', type=int, default = -999, help='# of characters to truncate from the left of read_2. Read truncation is not automatic, set accordingly. Default = 0.')\n",
    "# parser.add_argument('--purity', type=float, default = 0.95, help='minimum percentage of reads with the same beginning to be cut.')\n",
    "# parser.add_argument('--check_length', type=int, default=500, help='# of rows on the bottom and top of the array to check for concordance of starting letter.')\n",
    "# parser.add_argument('--check_reverse', action='store_true', help='# match reads against both guide1+reverse_comp(guide2) and guide2_reverse_comp(guide1)')\n",
    "# \n",
    "# args = parser.parse_args()\n",
    "# \n",
    "# if(args.packages):\n",
    "#   print(\"Must have numpy and pandas available. \\n Additionally, must install biopython.\")\n",
    "#   print(\"To install biopython, run pip install biopython, or, if using conda,\")\n",
    "#   print(\"conda install -c conda-forge biopython\")\n",
    "#   quit()\n",
    "# \n",
    "# print(\"#\"*46)\n",
    "# print(\"\")\n",
    "# print(\"Here is some basic info on the command you are about to run.\")\n",
    "# print(\"Python version info...\")\n",
    "# print(sys.version)\n",
    "# print(\"CLI argument info...\")\n",
    "# print(\"The guides file you are using is\", args.guides_file, \".\")\n",
    "# print(\"The r1 file you are using is\", args.r1_file, \".\")\n",
    "# print(\"The r2 file you are using is\", args.r2_file, \".\")\n",
    "# print(\"How many read groups are only for a quick test and not the full set?\", args.N_reads, \".\")\n",
    "# print(\"\")\n",
    "# print(\"#\"*46)\n",
    "# \n",
    "# guides_file = args.guides_file\n",
    "# r1_file = args.r1_file\n",
    "# r2_file = args.r2_file\n",
    "# N_reads = args.N_reads\n",
    "# N_rows = args.N_reads\n",
    "# guide_1_offset = args.guide_1_offset\n",
    "# guide_2_offset = args.guide_2_offset\n",
    "# read_1_offset = args.read_1_offset\n",
    "# read_2_offset = args.read_2_offset\n",
    "# check_length = args.check_length\n",
    "# purity = args.purity\n",
    "# check_reverse = args.check_reverse\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "# %tb"
   ],
   "id": "499f6d8ff1e1bf6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% 1. Import data\n",
    "\"\"\"# 1. Import data, this includes concensus guides and R1 + R2 fastqs.\"\"\"\n",
    "\n",
    "# Function to read the guide library file in .csv and .txt format\n",
    "# TODO: Ensure function runs\n",
    "# def read_guides_file(guides_file):\n",
    "#     file_extension = os.path.splitext(guides_file)[1].lower()\n",
    "#\n",
    "#     if file_extension == '.txt':\n",
    "#         # Read as a tab-delimited file\n",
    "#         guides_df = pd.read_csv(guides_file, sep = '\\t', engine = 'c')\n",
    "#     elif file_extension == '.csv':\n",
    "#         # Read as a comma-separated file\n",
    "#         guides_df = pd.read_csv(guides_file, engine = 'c')\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported file type: {}\".format(file_extension))\n",
    "#\n",
    "#     return guides_df\n",
    "#\n",
    "# guides_df = read_guides_file(guides_file)\n",
    "\n",
    "# Modify based on guide file format - Guide file received was in txt format\n",
    "guides_df = pd.read_csv(guides_file, sep='\\t')\n",
    "\n",
    "\"\"\"# # Import R1s and R2s.\n",
    "# ## pysam way Pysam introduces many other file format compatibilities such as\n",
    "# ## CRAM/SAM/BAM\n",
    "# if (N_rows == 0):\n",
    "#   with pysam.FastxFile(r1_file) as fh:\n",
    "#     r1_df = pd.DataFrame([(entry.name, entry.sequence, entry.comment, \\\n",
    "#     entry.quality) for entry in fh], columns=['name','seq', 'comment', 'qual'])\n",
    "#   with pysam.FastxFile(r2_file) as fh:\n",
    "#     r2_df = pd.DataFrame([(entry.name, entry.sequence, entry.comment, \\\n",
    "#     entry.quality) for entry in fh], columns=['name','seq', 'comment', 'qual'])\n",
    "# else:\n",
    "#   with pysam.FastxFile(r1_file) as fh:\n",
    "#     r1_df = pd.DataFrame([(entry.name, entry.sequence, entry.comment, \\\n",
    "#     entry.quality) for entry in islice(fh,0,N_rows)],\n",
    "#            columns=['name','seq', 'comment', 'qual'])\n",
    "#   with pysam.FastxFile(r2_file) as fh:\n",
    "#     r2_df = pd.DataFrame([(entry.name, entry.sequence, entry.comment, \\\n",
    "#     entry.quality) for entry in islice(fh,0,N_rows)],\n",
    "#     columns=['name','seq', 'comment', 'qual'])\n",
    "#\n",
    "# ## SeqIO way\n",
    "# For more compatitibility with other files types will need to import as SeqIO objects.\n",
    "# Consider the following suggestions if so.\n",
    "# use to_dict for compatibility with more file types and for true dictionary\n",
    "# functionality. If files too big, use .index.\n",
    "# Otherwise, if more memory needed, instantiate as a list \"\"\"\n",
    "\n",
    "# With open the gzip files (r1 and r2) in reading mode ('rt') and assign it to variable ('r1' and 'r2', respectively).\n",
    "# Initialize 'r1_it' iterator to iterate over contents in the first file with 'FastqqGeneralIterator'\n",
    "# Use of backslash ('\\') is a continuation of the 'with' statement across multiple lines of code for better readability\n",
    "# Check 'N_rows' is empty. Then create new dataframe from iterator. Dataframe has three columns.\n",
    "# Slice 'r1_it' and 'r2_it' from (0) to 'N_rows'. Then create dataframes from the sliced iterator\n",
    "with gzip.open(r1_file, mode = 'rt') as r1, \\\n",
    "     gzip.open(r2_file, mode = 'rt') as r2:\n",
    "  r1_it = FastqGeneralIterator(r1)\n",
    "  r2_it = FastqGeneralIterator(r2)\n",
    "\n",
    "  if (N_rows == 0):\n",
    "    r1_df = pd.DataFrame(r1_it, columns=['title', 'seq', 'qual'])\n",
    "    r2_df = pd.DataFrame(r2_it, columns=['title', 'seq', 'qual'])\n",
    "  else:\n",
    "    r1_df = pd.DataFrame(islice(r1_it, 0, N_rows),\n",
    "                         columns=['title', 'seq', 'qual'])\n",
    "    r2_df = pd.DataFrame(islice(r2_it, 0, N_rows),\n",
    "                         columns=['title', 'seq', 'qual'])\n",
    "\n",
    "# How many reads are in the raw sequencing files?\n",
    "total_r1_df = len(r1_df)\n",
    "total_r2_df = len(r2_df)\n",
    "# Get the index of all sequences with at least one unacceptable quality base\n",
    "# Access 'seq' column in 'r1_df' and check each string to see it contains the character 'N'. Returns bool\n",
    "removed_r1 = r1_df.seq.str.contains('N')\n",
    "removed_r2 = r2_df.seq.str.contains('N')\n",
    "\n",
    "all_removed = removed_r1 | removed_r2\n",
    "\n",
    "r1_df = r1_df.loc[~all_removed,:]\n",
    "r2_df = r2_df.loc[~all_removed,:]\n",
    "\n",
    "# Check the composition of the guides and the reads. Do they start with G?\n",
    "# How long are the reads?\n",
    "guide_1_length = len(guides_df.iloc[0,guides_df.columns.get_loc('protospacer_A')])\n",
    "guide_2_length = len(guides_df.iloc[0,guides_df.columns.get_loc('protospacer_B')])\n",
    "original_read_1_length = len(r1_df.iloc[0,r1_df.columns.get_loc('seq')])\n",
    "original_read_2_length = len(r2_df.iloc[0,r1_df.columns.get_loc('seq')])\n",
    "\n",
    "assert original_read_1_length >= guide_1_length, 'We must assume read 1 is at least as large as the guide. Otherwise, we did something wrong'\n",
    "assert original_read_2_length >= guide_2_length, 'We must assume read 2 is at least as large as the guide. Otherwise, we did something wrong'\n",
    "\n",
    "# Possible output for later, or usage here. The first letter composition of each thing\n",
    "guide_1_frst_ltrs = guides_df.loc[0:check_length,'protospacer_A'].astype(str).str[0].value_counts()\n",
    "guide_2_frst_ltrs = guides_df.loc[0:check_length,'protospacer_B'].astype(str).str[0].value_counts()\n",
    "r1_frst_ltrs = r1_df.loc[0:check_length,'seq'].astype(str).str[0].value_counts()\n",
    "r2_frst_ltrs = r2_df.loc[0:check_length,'seq'].astype(str).str[0].value_counts()\n",
    "\n",
    "print(\"Guide and read truncation readout: ##########\")\n",
    "print(f\"Length of input guide 1: {guide_1_length}\")\n",
    "print(f\"Length of input guide 2: {guide_2_length}\")\n",
    "print(f\"Length of input read 1: {original_read_1_length}\")\n",
    "print(f\"Length of input read 2: {original_read_2_length}\")\n",
    "\n",
    "guide_1_end = guide_1_length\n",
    "guide_2_end = guide_2_length\n",
    "\n",
    "def get_offset(df, c_length, purity, column, ltr):\n",
    "  \"\"\"\n",
    "  Get the offset based on the inputs.\n",
    "\n",
    "  Parameters\n",
    "  df - the dataframe to examine\n",
    "  c_length - the number of rows to check for the letter\n",
    "  purity - minimum percentage of reads that start with ltr\n",
    "  column - the column to look in\n",
    "  ltr - the starting letter to check for\n",
    "\n",
    "  Returns\n",
    "  offset_value - will be either 0 or 1\n",
    "  \"\"\"\n",
    "  first_gs = df.iloc[:c_length, df.columns.get_loc(column)]\n",
    "  last_gs = df.iloc[-c_length:, df.columns.get_loc(column)]\n",
    "  # Check whether all rows start with 'G'\n",
    "  first = sum(first_gs.str.startswith(ltr))\n",
    "  last = sum(last_gs.str.startswith(ltr))\n",
    "  if (((first + last) / (2 * c_length)) >= purity):\n",
    "    offset_value = 1\n",
    "  else:\n",
    "    offset_value = 0\n",
    "  return offset_value\n",
    "\n",
    "# If the user did not guide_1/read_1 offsets\n",
    "if guide_1_offset == -999: # -999 is the sentinel value meaning unset\n",
    "  # If the guide_1 all start with 'G'\n",
    "  guide_1_offset = get_offset(guides_df, check_length, purity, 'protospacer_A', 'G')\n",
    "  guide_1_length = (guide_1_end - guide_1_offset)\n",
    "\n",
    "if guide_2_offset == -999: # -999 is the sentinel value meaning unset\n",
    "  # If the guide_2 all start with 'G'\n",
    "  guide_2_offset = get_offset(guides_df, check_length, purity, 'protospacer_B', 'G')\n",
    "  guide_2_length = guide_2_end - guide_2_offset\n",
    "\n",
    "if read_1_offset == -999: # -999 is the sentinel value meaning unset\n",
    "  # If the read_1 all start with 'G'\n",
    "  read_1_offset = get_offset(r1_df, check_length, purity, 'seq', 'G')\n",
    "\n",
    "if read_2_offset == -999: # -999 is the sentinel value meaning unset\n",
    "  # If the read_2 all start with 'G'\n",
    "  read_2_offset = get_offset(r2_df, check_length, purity, 'seq', 'C')\n",
    "\n",
    "# Check everything\n",
    "if read_1_offset + guide_1_length > original_read_1_length:\n",
    "  raise IndexError(\"Read 1 not long enough to sustain truncation. \" + \\\n",
    "                   \"Check input and parameters or manually set offsets.\")\n",
    "if read_2_offset + guide_2_length > original_read_2_length:\n",
    "  raise IndexError(\"Read 2 not long enough to sustain truncation \" + \\\n",
    "                   \"Check input and parameters or manually set offsets.\")\n",
    "\n",
    "# set read lengths based on guide lengths. As above, if the reads are too short\n",
    "# there is something wrong with the input and things should be rethought.\n",
    "# As long as the read is longer than the guide, the below operations should be\n",
    "# safe.\n",
    "\n",
    "read_1_length = guide_1_length\n",
    "read_1_end = read_1_offset + read_1_length\n",
    "\n",
    "read_2_length = guide_2_length\n",
    "read_2_end = read_2_offset + read_2_length\n",
    "\n",
    "if read_1_end - read_1_offset > guide_1_length:\n",
    "  raise IndexError(f\"After truncation, read 1 length ({str(read_1_length)})\" + \\\n",
    "                  f\"is too big! guide 1 length ({str(guide_1_length)})!\")\n",
    "if read_2_end - read_2_offset > guide_2_length:\n",
    "  raise IndexError(f\"After truncation, read 2 length ({str(read_2_length)})\" + \\\n",
    "                    f\"is too big! guide 2 length ({str(guide_2_length)})!\")\n",
    "\n",
    "print(f\"Therefore, guide 1 is now {guide_1_length} bp and guide 2 is {guide_2_length} bp\")\n",
    "print(f\"guide_1: Going from {guide_1_offset} to {guide_1_end} and guide_2:\" + \\\n",
    "      f\" from {guide_2_offset} to {guide_2_end}\")\n",
    "print(f\"read_1: Going from {read_1_offset} to {read_1_end} and read_2\" + \\\n",
    "      f\" from {read_2_offset} to {read_2_end}\")\n",
    "print(\"The reads have been truncated to be the same size as the guides.\")\n",
    "\n",
    "assert read_1_length == guide_1_length, 'Oh no, fix read_1 and g1!'\n",
    "assert read_2_length == guide_2_length, 'Oh no, fix read_2 and g2!'\n",
    "assert read_1_end - read_1_offset == read_1_length, 'Oh no, fix read_1 and g2!'\n",
    "assert read_2_end - read_2_offset == read_2_length, 'Oh no, fix read_2 and g2!'\n",
    "assert guide_1_length == guide_1_end - guide_1_offset, 'Oh no, fix guide_1!'\n",
    "assert guide_2_length == guide_2_end - guide_2_offset, 'Oh no, fix guide_2!'\n",
    "\n",
    "# only do this once\n",
    "r1_df.insert(loc=2, column='plus', value='+')\n",
    "r2_df.insert(loc=2, column='plus', value='+')\n",
    "\n",
    "# Function to split the sequence header from the sequence ID and returns the sequence header. The sequence header contains info about the run and tile coordinates. The sequence ID contains the guide sequence in addition to other info\n",
    "def split_str(s: str) -> str:\n",
    "    return s.split(\" \", maxsplit = 1)[0]\n",
    "# Function to split the sequence header from the sequence ID and returns the sequence header. The sequence header contains info about the run and tile coordinates. The sequence ID contains the guide sequence in addition to other info\n",
    "# Create new 'read_group' column. All values in read_group should be unique\n",
    "r1_df['read_group'] = r1_df[\"title\"].apply(split_str)\n",
    "r2_df['read_group'] = r2_df[\"title\"].apply(split_str)\n",
    "\n",
    "r1_df['title'] = '@' + r1_df['title']\n",
    "r2_df['title'] = '@' + r2_df['title']\n",
    "\n",
    "\"\"\"This next code block defines the r1 and r2 keys that are often 19 BP, also making a reverse complement of r2 key.\"\"\"\n",
    "\n",
    "# New as of v5.\n",
    "# Create new column, 'protospacer_A_19bp_trimmed', to contain a portion of the original 'protospacer_A' sequence between guide_1_offset and guide_1_end. Also applies to 'protospacer_B'\n",
    "guides_df['protospacer_A_19bp_trimmed'] = [x[guide_1_offset:guide_1_end] \\\n",
    "                                           for x in guides_df['protospacer_A']]\n",
    "guides_df['protospacer_B_19bp_trimmed'] = [x[guide_2_offset:guide_2_end] \\\n",
    "                                           for x in guides_df['protospacer_B']]\n",
    "\n",
    "# Make guide key columns.\n",
    "guides_df['r1_key'] = guides_df['protospacer_A_19bp_trimmed']\n",
    "\n",
    "# R2 is tricky as it is the reverse complement. Flip it and translate using the function from Bio pkg below.\n",
    "guides_df['r2_key'] = guides_df['protospacer_B_19bp_trimmed'].apply(reverse_complement)\n",
    "guides_df['r1_r2_key'] = guides_df['r1_key'] + \"_\" + guides_df['r2_key']\n",
    "\n",
    "# Include an option to do the reverse as well\n",
    "if check_reverse:\n",
    "  guides_df['r2_r1_key'] = guides_df['protospacer_B_19bp_trimmed'] + \"_\" + guides_df['protospacer_A_19bp_trimmed'].apply(reverse_complement)\n",
    "\n",
    "# Get the guide seqs, these relate to the 19 BP segments in the guide_df.\n",
    "# Iterate through each element in 'x' in the r1_df['seq'] column, then extract the sequence from read_1_offset to read_1_end and store values in a new column, 'guide_seq'\n",
    "r1_df.loc[:,'guide_seq'] = [x[read_1_offset:read_1_end] for x in r1_df.seq]\n",
    "r2_df.loc[:,'guide_seq'] = [x[read_2_offset:read_2_end] for x in r2_df.seq]\n"
   ],
   "id": "73cad33da92a0924"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% 2. Identify matching read groups\n",
    "\n",
    "\"\"\"# 2. Identify matching read groups across R1 and R2.\"\"\"\n",
    "\n",
    "# Pull the read groups from R1 and R2. Make a consensus read group list.\n",
    "r1_read_groups_df = r1_df[['read_group']]\n",
    "r2_read_groups_df = r2_df[['read_group']]\n",
    "\n",
    "# Consensus contains all read groups from R1 and R2 files\n",
    "consensus_read_groups_df = r1_read_groups_df.merge(r2_read_groups_df, on ='read_group', how='inner')\n",
    "\n",
    "# Quantify obvious pair failures.\n",
    "r1_N_attempted_read_groups = r1_df.read_group.shape[0]\n",
    "r2_N_attempted_read_groups = r1_df.read_group.shape[0]\n",
    "consensus_N_read_groups = consensus_read_groups_df.shape[0]\n",
    "r1_pcnt_consensus = round((consensus_N_read_groups/r1_N_attempted_read_groups)*100, 2)\n",
    "r2_pcnt_consensus = round((consensus_N_read_groups/r2_N_attempted_read_groups)*100, 2)\n",
    "all_removed_pct = round((sum(all_removed)/(total_r1_df + total_r2_df))*100, 2)\n",
    "\n",
    "print(\"#\"*46)\n",
    "print(f\"R1 had {r1_N_attempted_read_groups} potential read groups, of these {r1_pcnt_consensus}% were among the concensus read groups. \\n\"\n",
    "      f\"R2 had {r2_N_attempted_read_groups} potential read groups, of these {r2_pcnt_consensus}% were among the concensus read groups. \\n\"\n",
    "      f\"In total there are {consensus_N_read_groups} read groups that are matching across R1 and R2 for this experiment.\")\n",
    "print(f\"{sum(all_removed)} ({all_removed_pct}%) potential read groups were removed from the raw FASTQ files for poor quality reads\")\n",
    "print(\"#\"*46)\n"
   ],
   "id": "d63c72fed24fc2be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% 3. Reduce R1 and R2 to only those in the concensus_read_groups df.\n",
    "\"\"\"# 3. Reduce the datasets to the read groups that match in R1 and R2.\n",
    "\"\"\"\n",
    "\n",
    "# Reduce R1 and R2 to only those in the concensus_read_groups df.\n",
    "consensus_read_list = consensus_read_groups_df.read_group.unique()\n",
    "\n",
    "r1_df['in_consensus'] = r1_df.read_group.isin(consensus_read_list)\n",
    "r2_df['in_consensus'] = r2_df.read_group.isin(consensus_read_list)\n",
    "\n",
    "r1_reduced_df = r1_df[r1_df['in_consensus'] == True]\n",
    "r2_reduced_df = r2_df[r1_df['in_consensus'] == True]\n"
   ],
   "id": "9f90045204a210bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% 4. Split data into two dataframes\n",
    "\"\"\"# 4. Split into 'hits.\\'  and 'recombinants.\\'.  \n",
    "'hits.\\' denotes read group matches and the protospacers match.\n",
    "'recombinants.\\' denotes read group matches but one or more protospacers does not.\n",
    "\"\"\"\n",
    "\n",
    "# Build dataset that is used to check for recombination w/in each read group.\n",
    "r1_guide_read_df = r1_reduced_df[['read_group', 'guide_seq']].copy()\n",
    "r1_guide_read_df.rename(columns={'guide_seq': 'r1_guide_seq'}, inplace=True)\n",
    "\n",
    "r2_guide_read_df = r2_reduced_df[['read_group', 'guide_seq']].copy()\n",
    "r2_guide_read_df.rename(columns={'guide_seq': 'r2_guide_seq'}, inplace=True)\n",
    "\n",
    "# Merge R1\n",
    "combined_guide_read_df = r1_guide_read_df.merge(r2_guide_read_df, on='read_group', how='inner')\n",
    "combined_guide_read_df['combined_guide_seqs'] = combined_guide_read_df['r1_guide_seq'] + \"_\" + combined_guide_read_df['r2_guide_seq']\n",
    "\n",
    "# Flag expected pairs from the guides_df.\n",
    "reference_list = guides_df['r1_r2_key'].tolist()\n",
    "\n",
    "if check_reverse:\n",
    "  reference_list.extend(guides_df['r2_r1_key'].tolist())\n",
    "\n",
    "uppercase_reference_list = [x.upper() for x in reference_list]\n",
    "\n",
    "combined_guide_read_df['uppercase_combined_guide_seqs'] = combined_guide_read_df['combined_guide_seqs'].str.upper()\n",
    "\n",
    "combined_guide_read_df['non_recombinant'] = combined_guide_read_df['uppercase_combined_guide_seqs'].isin(uppercase_reference_list)\n",
    "\n",
    "# TODO: differentiate commented try statment vs uncommented try statement. Commented version was originally in v10, uncommented version was orignally in v8 (19_20)\n",
    "# # Counting hits\n",
    "try:\n",
    "    # Count the number of non-recombinant values where non_recombinant = TRUE (ie count values that matched guides in library)\n",
    "    on_target = combined_guide_read_df['non_recombinant'].value_counts()[True]\n",
    "except KeyError:\n",
    "    warnings.warn(\"WARNING: There are no on-target hits. Something is probably wrong.\",\n",
    "                  category = RuntimeWarning)\n",
    "    on_target = 0\n",
    "\n",
    "# Counting recombinants\n",
    "try:\n",
    "    recombinant = combined_guide_read_df['non_recombinant'].value_counts()[False]\n",
    "except KeyError:\n",
    "    warnings.warn(\"WARNING: There are no recombinant hits. Something is probably wrong.\",\n",
    "                  category = RuntimeWarning)\n",
    "    recombinant = 0\n",
    "\n",
    "# # Try statements in v8\n",
    "# try:\n",
    "#     # Use .value_counts().iloc[1] to get the second most frequent True/False value. .iloc[1]=False\n",
    "#     on_target = combined_guide_read_df['non_recombinant'].value_counts().iloc[1]\n",
    "# except IndexError:  # Catching IndexError instead of KeyError because we're dealing with list-like access\n",
    "#     warnings.warn(\"WARNING: There are no on-target hits. Something is probably wrong.\", category=RuntimeWarning)\n",
    "#     # If fewer than 2\n",
    "#     on_target = 0\n",
    "#\n",
    "# try:\n",
    "#     # Use .value_counts().iloc[0] for positional indexing. .iloc[0]=True\n",
    "#     recombinant = combined_guide_read_df['non_recombinant'].value_counts().iloc[0]\n",
    "# except IndexError:  # Catching IndexError instead of KeyError because we're dealing with list-like access\n",
    "#     warnings.warn(\"WARNING: There are no recombinant hits. Something is probably wrong.\", category=RuntimeWarning)\n",
    "#     recombinant = 0\n",
    "\n",
    "total_reads = on_target + recombinant\n",
    "on_target_pcnt = round((on_target/total_reads)*100, 2)\n",
    "recombinant_pcnt = round((recombinant/total_reads)*100, 2)\n",
    "\n",
    "print(\"#\"*46)\n",
    "print(f\"There are a total of {total_reads} potential read groups after filtering, \\n\"\n",
    "      f\"Hits: {on_target} ({on_target_pcnt}%) of the {total_reads} were on target for R1 and R2. \\n\"\n",
    "      f\"This means {recombinant} ({recombinant_pcnt}%) of the {total_reads} are recombinant read groups.\")\n",
    "print(\"#\"*46)\n",
    "print('\\n')\n",
    "\n",
    "if(on_target_pcnt < 25):\n",
    "  warnings.warn(f\"Not many hits. Check if guides and reads were trimmed \" + \\\n",
    "                f\"appropriately. \\nread_1 comp: \\n{r1_frst_ltrs} \\n\" + \\\n",
    "                f\"read_2 comp: \\n{r2_frst_ltrs} \\n \" + \\\n",
    "                f\"guide_1 comp: \\n{guide_1_frst_ltrs} \\n \" + \\\n",
    "                f\"guide_2 comp: \\n{guide_2_frst_ltrs} \")\n",
    "\n",
    "# Split data into recombinant and hit subsets.\n",
    "# Select read groups where combined_guide_read_df['non_recombinant'] == True\n",
    "hits_df = combined_guide_read_df[combined_guide_read_df['non_recombinant'] == True][['read_group']]\n",
    "# Select read groups where combined_guide_read_df['non_recombinant'] == False\n",
    "recombinant_df = combined_guide_read_df[combined_guide_read_df['non_recombinant'] == False][['read_group']]\n",
    "\n",
    "hits_list = hits_df.read_group.tolist()\n",
    "\n",
    "r1_reduced_df['hit'] = r1_reduced_df['read_group'].isin(hits_list)\n",
    "r2_reduced_df['hit'] = r2_reduced_df['read_group'].isin(hits_list)\n",
    "\n",
    "r1_hits_df = r1_reduced_df[r1_reduced_df['hit'] == True]\n",
    "r1_recombinant_df = r1_reduced_df[r1_reduced_df['hit'] == False]\n",
    "\n",
    "r2_hits_df = r2_reduced_df[r2_reduced_df['hit'] == True]\n",
    "r2_recombinant_df = r2_reduced_df[r2_reduced_df['hit'] == False]"
   ],
   "id": "ca841c0ff29dc451"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% 5. Export two dfs per fastq\n",
    "\"\"\"# 5. Export 'hits.\\' and 'recombinants.\\' per fastq.\n",
    "\"\"\"\n",
    "\n",
    "# Now its just back to the fastqs from here ... ouch. Start stacking the hits!\n",
    "r1_hits_stacked_df = r1_hits_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "r2_hits_stacked_df = r2_hits_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "\n",
    "r1_hits_fastq_df = r1_hits_stacked_df\n",
    "r2_hits_fastq_df = r2_hits_stacked_df\n",
    "\n",
    "# Identify fails, these are recombinants with either R1 or R2 not in the guide list. If the uppercase guide sequences are not in a match to uppercase r1_key or r2_key from guides_df.\n",
    "uppercase_r1_keys = [x.upper() for x in guides_df['r1_key']]\n",
    "uppercase_r2_keys = [x.upper() for x in guides_df['r2_key']]\n",
    "\n",
    "r1_recombinant_df = r1_recombinant_df.copy()  # Ensure we're working with a copy\n",
    "r1_recombinant_df['uppercase_guide_seq'] = r1_recombinant_df['guide_seq'].str.upper()\n",
    "r2_recombinant_df = r2_recombinant_df.copy()  # Ensure we're working with a copy\n",
    "r2_recombinant_df['uppercase_guide_seq'] = r2_recombinant_df['guide_seq'].str.upper()\n",
    "\n",
    "# r1_recombinant_df = r1_recombinant_df.copy()  # Ensure it's a copy, not a view\n",
    "r1_recombinant_df.loc[:, 'in_guide_library'] = r1_recombinant_df['uppercase_guide_seq'].isin(uppercase_r1_keys)\n",
    "# r2_recombinant_df = r1_recombinant_df.copy()  # Ensure it's a copy, not a view\n",
    "r2_recombinant_df.loc[:, 'in_guide_library'] = r2_recombinant_df['uppercase_guide_seq'].isin(uppercase_r2_keys)\n",
    "# FIXME: seqs in r2 recomb df dont match anything in r2 keys but ~480 match r1 key\n",
    "\n",
    "# TODO: consider adding function for user input to select permissiveness of output files\n",
    "    # TODO: matches r1 sequence\n",
    "    # TODO: matches r2 sequence\n",
    "\n",
    "\"\"\"# Split R1 and R2 into true recombinants and failed recombinants based on the 'in_guide_library' flag. But first make a list of read groups that fail. Then pull these readgroups to split true recombinants and fails.\n",
    "\"\"\"\n",
    "# May be due to recombination or sequence read error, need to differentate between the two reasons\n",
    "r1_failed_recombinants = r1_recombinant_df[r1_recombinant_df['in_guide_library'] == False]\n",
    "r2_failed_recombinants = r2_recombinant_df[r2_recombinant_df['in_guide_library'] == False]\n",
    "# TODO: add function to calculate hamming distance and tolerate nucleotide substitution of a user-specified value\n",
    "# recombinant_failed_readgroups = pd.concat((r1_failed_recombinants['read_group'],\n",
    "#                                           r2_failed_recombinants['read_group']),\n",
    "#                                           axis=0\n",
    "#                                           ).unique()\n",
    "recombinant_failed_readgroups = pd.concat((r1_failed_recombinants['read_group'],\n",
    "                                          r2_failed_recombinants['read_group']),\n",
    "                                          axis=0\n",
    "                                          ).unique()"
   ],
   "id": "e4a0de526682f0ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "N_big_fails = len(recombinant_failed_readgroups)\n",
    "# FIXME: The sum of reads not in the guide library(len(r1_failed_recombinants) + len(r2_failed_recombinants)  should equal to N_big_fails = len(recombinant_failed_readgroups) since those sequences were not in the guide library\n",
    "    # remember that uppercase_r2_keys maps to the reverse complement of protospacer B\n",
    "print(f'Number of R1 failed recombinants: {len(r1_failed_recombinants)}')\n",
    "print(f'Number of R2 failed recombinants: {len(r2_failed_recombinants)}')\n",
    "# print(f'The sum of R1 and R2 failed recombinants: {len(r1_failed_recombinants) + len(r2_failed_recombinants)}')\n",
    "print(f'Number of failed read groups: {N_big_fails}')\n",
    "\n",
    "# Creates a new column to flag if read groups are in 'recombinant_failed_readgroups'. If yes, then 'big_fail' == True, otherwise False\n",
    "r1_recombinant_df = r1_recombinant_df.copy()  # Ensure it's a copy, not a view\n",
    "r1_recombinant_df.loc[:, 'big_fail'] = r1_recombinant_df.loc[:, 'read_group'].isin(recombinant_failed_readgroups)\n",
    "r2_recombinant_df = r2_recombinant_df.copy()  # Ensure it's a copy, not a view\n",
    "r2_recombinant_df.loc[:, 'big_fail'] = r2_recombinant_df.loc[:, 'read_group'].isin(recombinant_failed_readgroups)\n",
    "\n",
    "# FIXME: weird... there are 58 rows in r1_recombinant_df that are in the guide library AND a big fail... how?\n",
    "\n",
    "# Read groups/guide seq are in 'recombinant_failed_readgroups'... TODO: Consider sequencing base error?\n",
    "r1_failed_recombinants_df = r1_recombinant_df[r1_recombinant_df.loc[:, 'big_fail'] == True]\n",
    "r2_failed_recombinants_df = r2_recombinant_df[r2_recombinant_df.loc[:, 'big_fail'] == True]\n",
    "r1_true_recombinants_df = r1_recombinant_df[r1_recombinant_df.loc[:, 'big_fail'] == False]\n",
    "r2_true_recombinants_df = r2_recombinant_df[r2_recombinant_df.loc[:, 'big_fail'] == False]\n",
    "# r1_recombinant_df[r1_recombinant_df['in_guide_library'] == True]\n",
    "\n",
    "big_fail_pcnt = round((N_big_fails/recombinant)*100, 2)\n",
    "\n",
    "num_r1_failed_recombinants = r1_failed_recombinants['read_group']\n",
    "num_r2_failed_recombinants = r2_failed_recombinants['read_group']\n",
    "\n",
    "print(\"#\"*46)\n",
    "print(f\"Of the {recombinant} recombinant read groups, {N_big_fails} read groups had a sequence not in the guide list, so {big_fail_pcnt} % of recombinants can be considered failures.\")\n",
    "# TODO: no way 100% of recombinant read groups are failed reads\n",
    "print(\"#\"*46)"
   ],
   "id": "f3e25a064a59ec1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Start stacking the recombinants!\n",
    "r1_failed_recombinant_stacked_df = r1_failed_recombinants_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "r2_failed_recombinant_stacked_df = r2_failed_recombinants_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "\n",
    "r1_failed_recombinant_fastq_df = r1_failed_recombinant_stacked_df\n",
    "r2_failed_recombinant_fastq_df = r2_failed_recombinant_stacked_df\n",
    "\n",
    "r1_true_recombinant_stacked_df = r1_true_recombinants_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "#new line below\n",
    "r2_true_recombinants_df['seq'] = r2_true_recombinants_df['seq'].map(lambda x: swap_r1_r2_seq(x))\n",
    "r2_true_recombinant_stacked_df = r2_true_recombinants_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "\n",
    "r1_true_recombinant_fastq_df = r1_true_recombinant_stacked_df\n",
    "r2_true_recombinant_fastq_df = r2_true_recombinant_stacked_df"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#new lines: swap sequences\n",
    "def swap_r1_r2_seq(r2_sequence):\n",
    "    row = guides_df[guides_df['r2_key'] == r2_sequence[0:19].upper()]\n",
    "    return row.iloc[0]['r1_key']\n",
    "\n",
    "# Start stacking the recombinants!\n",
    "r1_failed_recombinant_stacked_df = r1_failed_recombinants_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "r2_failed_recombinant_stacked_df = r2_failed_recombinants_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "\n",
    "r1_failed_recombinant_fastq_df = r1_failed_recombinant_stacked_df\n",
    "r2_failed_recombinant_fastq_df = r2_failed_recombinant_stacked_df\n",
    "\n",
    "r1_true_recombinant_stacked_df = r1_true_recombinants_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "#new line below\n",
    "r2_true_recombinants_df['seq'] = r2_true_recombinants_df['seq'].map(lambda x: swap_r1_r2_seq(x))\n",
    "r2_true_recombinant_stacked_df = r2_true_recombinants_df[['title', 'seq', 'plus', 'qual']].stack()\n",
    "\n",
    "r1_true_recombinant_fastq_df = r1_true_recombinant_stacked_df\n",
    "r2_true_recombinant_fastq_df = r2_true_recombinant_stacked_df"
   ],
   "id": "68053a4a9cd3e104"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Export the new fastq.\n",
    "# Define the child directory relative to the current script's location. Use lines for shell script\n",
    "# script_dir = os.path.dirname(os.path.abspath(__file__))  # Absolute path to the script\n",
    "# child_directory = os.path.join(script_dir, 'output')\n",
    "\n",
    "# Define the child directory relative to the current working directory\n",
    "child_directory = os.path.join(os.getcwd(), 'output')\n",
    "\n",
    "# Ensure the child directory exists\n",
    "if not os.path.exists(child_directory):\n",
    "    os.makedirs(child_directory)\n",
    "\n",
    "# Extract just the filename from r1_file and r2_file\n",
    "r1_filename = os.path.basename(r1_file)\n",
    "r2_filename = os.path.basename(r2_file)\n",
    "\n",
    "print(f\"Saving files to directory: {child_directory}\")\n",
    "\n",
    "# Now set the output file paths in the subdirectory 'hits'\n",
    "r1_hits_out_file = os.path.join(child_directory, \"hits.\" + r1_filename)\n",
    "r2_hits_out_file = os.path.join(child_directory, \"hits.\" + r2_filename)\n",
    "r1_true_recombinant_out_file = os.path.join(child_directory, \"recombinants.\" + r1_filename)\n",
    "r2_true_recombinant_out_file = os.path.join(child_directory, \"recombinants.\" + r2_filename)\n",
    "r1_failed_recombinant_out_file = os.path.join(child_directory, \"fails.\" + r1_filename)\n",
    "r2_failed_recombinant_out_file = os.path.join(child_directory, \"fails.\" + r2_filename)\n",
    "\n",
    "# Save the DataFrames to their respective CSV files\n",
    "r1_hits_fastq_df.to_csv(r1_hits_out_file, sep='\\t', index=False, header=False, compression='gzip')\n",
    "r2_hits_fastq_df.to_csv(r2_hits_out_file, sep='\\t', index=False, header=False, compression='gzip')\n",
    "r1_true_recombinant_fastq_df.to_csv(r1_true_recombinant_out_file, sep='\\t', index=False, header=False, compression='gzip')\n",
    "r2_true_recombinant_fastq_df.to_csv(r2_true_recombinant_out_file, sep='\\t', index=False, header=False, compression='gzip')\n",
    "r1_failed_recombinant_fastq_df.to_csv(r1_failed_recombinant_out_file, sep='\\t', index=False, header=False, compression='gzip')\n",
    "r2_failed_recombinant_fastq_df.to_csv(r2_failed_recombinant_out_file, sep='\\t', index=False, header=False, compression='gzip')\n"
   ],
   "id": "d01c0b791f85835e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"#\"*46)\n",
    "print(\"Your analysis has just finished.\")\n",
    "print(\"Reads from matched read groups on whose guides were on target for both R1 and R2 are found in the files prefixed hits.*.\")\n",
    "print(\"Reads from matched read groups on whose guides were on were off target and considered recombinant for either R1 and R2 are found in the files prefixed recombinants.*.\")\n",
    "print(\"Reads from recombinant read groups on whose guides were not mathced to a known guide sequence for either R1 and R2 are found in the files prefixed fails.*.\")\n",
    "print(\"Good luck and feel free to generally ignore any outputs below here!\")\n",
    "print(\"#\"*46)"
   ],
   "id": "bf56c4d1aebdf047"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
